## OBJECTIVE
Produce a counts tsv output file from the st_pipeline (Spatial Transcriptomics Pipeline - https://github.com/SpatialTranscriptomicsResearch/st_pipeline) tool for downstream analysis of fastq files. 

## WHAT YOU'LL NEED
- Access to the folder at /gpfs/gibbs/pi/b_hafler/OpticNerveHead_DBiT, where we will use the scripts in /gpfs/gibbs/pi/b_hafler/OpticNerveHead_DBiT/Analysis/Scripts for the steps later described.
- As required by the above, to quote from the st_pipeline documentation (at the aforementioned GitHub link):
	- FASTQ files (Read 1 containing the spatial information and the UMI and read 2 containing the genomic sequence)
	- A genome index generated with STAR
An annotation file in GTF or GFF3 format (optional when using a transcriptome)
	- The file containing the barcodes and array coordinates (look at the folder "ids" to use it as a reference). Basically this file contains 3 columns (BARCODE, X and Y), so if you provide this file with barcodes identinfying cells (for example), the ST pipeline can be used for single cell data. This file is also optional if the data is not barcoded (for example RNA-Seq data). 

## STEPS
All steps are submittable as SBATCH jobs.
- Prepare your environment. Load conda environment ENVIRONMENT_NAME. In a terminal window, run:
	- conda activate ENVIRONMENT_NAME
- Get your data. Fetch from novogene (wget download). Use link/command provided by novogene data download email.
- Prepare your data - fastq process on the second read
	- python ../Python/fastq_process.py $FW_orig in script
- Run st_pipeline
- Run convertoname.sh to get final expression matrix from st_pipeline output

Refer to the individual scripts, among the five scripts, to see per the comments where the file or folder paths may require replacement by you as a matter of a new run or a new user point of access.

In essence, you will be running five .sh scripts in sequence.
The five are to: get the data, make the STAR referencei (anticipate at least 20 minutes of runtime on the cluster once this job is submitted), run the fastq process to strip the barcodes for each read and prepend them, run st_pipeline to get the counts.tsv file, and (not necessarily required by the newest versions of st_pipeline) run converttoname to account for gene names.

### FIRST, EDIT PATHS IN THE SCRIPTS
To edit files, you can open them directly in the terminal using vim or emacs, or you can edit them externally in a preferred text editor. The Ruddle dashboard, for example, allows for files to be downloaded and re-uploaded if you want to download a script file to edit locally before putting the new version back on Ruddle.

In 01_fetch_novogene_ftp.sh, change the USER and PASSWORD fields, without deleting the quotation marks surrounding each, to match the username and password provided by the email alerting of Novogene data being ready for download.

In 02_BuildSTARreference.sh, change the line #SBATCH --mail-user=XXX to replace XXX (just placeholder text here, for the sake of description) with your preferred email for being alerted of the eventual submitted job's status. Expect this job to take 1-2 hours, generally, once it's run in the next section.

In 03_Run_fastq_process.sh, change the line #SBATCH --mail-user=XXX to replace XXX with your preferred email for being alerted of the eventual submitted job's status. Replace FW_orig with the fastq files object path -- the path you replace with should end in fq.gz -- without deleting the quotation marks surrounding the path.

In 04_Run_stpipeline.sh, change the line #SBATCH --mail-user=XXX to replace XXX with your preferred email for being alerted of the eventual submitted job's status. Replace the RV and FW paths with your data directory fastq files ending in \_1.fq.gz and \_2_processed.fastq.gz, respectively. Ensure that the path for MAP matches the path generated by the 02_BuildSTARreference.sh job to your reference files subfolder inside the ReferenceFiles folder. Replace A22-1650_slide2 in PATH_TO_OUTPUT=`pwd`/../../data/A22-1650_slide2/processed/ to match the name of the data subfolder you desire to use (contains the data from having run 01_fetch_novogene_ftp.sh).

### SECOND, RUN THE SCRIPTS IN SEQUENCE

Open a Terminal window either in a Ruddle command-line interactive session or remote desktop, or ssh into Ruddle from a local Terminal/command-line window, and run (copy, then hit Enter) the following lines in sequence. Run these lines after you've already edited the scripts per the previous section.

```
sbatch 01_fetch_novogene_ftp.sh 
sbatch 02_BuildSTARreference.sh
sbatch 03_Run_fastq_process.sh
sbatch 04_Run_stpipeline.sh
```
